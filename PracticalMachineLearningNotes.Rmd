---
title: "Practical Machine Learning Notes"
author: "Coursera Course by John Hopkins University"
date: "INSTRUCTORS: Dr. Jeff Leek, Dr. Brian Caffo, Dr. Roger D. Peng"
fontsize: 11pt
output: 
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro  
* This course covers the basic ideas behind machine learning/prediction  
  + Study Design - training vs. test sets  
  + Conceptual issues - out of sample error, overfitting, ROC curves  
  + Practical Implementation - the caret package  
* What this course depends on:  
  + The Data Scientist's Toolbox  
  + R Programming  
* What would be useful  
  + Exploratory Analysis  
  + Reproducible Research  
  + Regression Models  
  + (Notes on these 5 courses are all in my GitHub repoes)  
  
## GitHub Link for Lectures  
**[Practical Machine Learning lectures on GitHub](https://github.com/bcaffo/courses/tree/master/08_PracticalMachineLearning)**  

## Course Book  
**[The book for this course is available on this site](https://web.stanford.edu/~hastie/ElemStatLearn//)**  

## Instructor's Note  
"*Welcome to Practical Machine Learning! This course will focus on developing the tools and techniques for understanding, building, and testing prediction functions.*  
  
*These tools are at the center of the Data Science revolution. Many researchers, companies, and governmental organizations would like to use the cheap and abundant data they are collecting to predict what customers will like, what services to offer, or how to improve people's lives.*  
  
*Jeff Leek and the Data Science Track Team*"  


# Prediction, Errors, and Cross Validation  
## Prediction  
### Prediction Motivation  
* Who predicts things?  
  + Local governments -> pension payments  
  + Google -> whether you will click on an ad  
  + Amazon -> what movies you will watch  
  + Insurance companies -> what your risk of death is  
  + Johns Hopkins -> who will succeed in their programs  
* Why predict things  
  + Glory (Nerd cred for accomplishing certain feats)  
    - A lot of competitions are hosted on **[Kaggle](http://www.kaggle.com/)**  
  + Riches (Completing some competition that offers a reward)  
  + Save lives  
    - **[On cotype DX](http://www.oncotypedx.com/en-US/Home)** reveals the underlying biology that changes treatment decisions 37% of the time.  

#### More Resources
* **[A course on more advanced material about ML](https://www.coursera.org/course/ml)**  
* **[List of machine learning resources on Quora](http://www.quora.com/Machine-Learning/What-are-some-good-resources-for-learning-about-machine-learning-Why)**  
* **[List of machine learning resources from Science](http://www.sciencemag.org/site/feature/data/compsci/machine_learning.xhtml)**  
* **[Advanced notes from MIT open courseware](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)**  
* **[Advanced notes from CMU](http://www.stat.cmu.edu/~cshalizi/350/)**  
* **[Kaggle - machine learning competitions](http://www.kaggle.com/)**  

### What is Prediction?  
#### Main Idea  
* One focus of ML is on what algorithms are the best for extracting information and using it to predict.  
* Although the method used for producing a training set is also quite important  
![Main Idea of ML](./Images/ML_Main_Idea.png)
  
* One starts off with a dataset  
1) One uses Probability/Sampling to select a Training Set  
2) One measures characteristics of this training set to create a Predicition Function  
3) One then uses the Prediction Function to take an uncolored dot and predict if it's red or blue  
4) One would then go on to test how well their Prediction Function works  

#### What Can Go Wrong  
* An example is **[Google Flu trends](http://www.sciencemag.org/content/343/6176/1203)** (**[A free overview of the issue witht he accuracy](https://en.wikipedia.org/wiki/Google_Flu_Trends#Accuracy)**)  
  - Google tried to predict rate of flu using what people would search  
  - Originally the algorithm was able to represent how many cases would appear in a region within a certain time  
  - Although they didn't account for the fact that the terms would change over time  
  - The way the terms were being used wasn't well understood so when terms changed they weren't able to accurately account for the change.  
  - It also overestimated as it the search terms it looked at were often cofactors with other illnesses  
  
#### Componets of a Predictor  
1) Question  
* Any problem in data science starts with a question, "What are you trying to predict and what are you trying to predict it with?"  

2) Input Data  
* Collect best input data you can to use to predict  

3) Features  
* From that data one builds features that they will use to predict  

4) Algorithm  
* One uses ML algorithms to develop a function  

5) Parameters  
* Estimate parameters of the algorithm  

6) Evaluation  
* Apply algorithm to a data set to evaluate how well the algorithm works  

#### Example  
* Start with a general question, "Can I automatically detect emails that are SPAM and those that are not?"  
* Make the question more concrete, "Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?"  
* Find input data  
  + In this instance there is data avaliable in R via the `kernlab` package  
  + Note that this data set won't necessarily be the perfect data as it doesn't contain all the emails ever sent, or the emails sent to you personally  
* Quantify features, such as the frequency of certain words or typeface. The `spam` dataset from `kernlab` contains these types of frequency.  
```{r}
library(kernlab)
data(spam)
str(spam)
```
```{r}
plot(density(spam$your[spam$type=="nonspam"]),
     col = "#5BC2E7", main = "", xlab = "Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]), col = "#FF0000")
```
  
 * It can be seen here "your" appears more often in SPAM emails than it does in HAM  
  + One could use this idea to create a cut-off point for predicting if a message is SPAM  
* The proposed algorithm  
  + Find a value of $C$  
  + If the frequency of $'your'>C$ predict the message is SPAM  
```{r}
plot(density(spam$your[spam$type=="nonspam"]),
     col = "#5BC2E7", main = "", xlab = "Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]), col = "#FF0000")
abline(v = 0.5, col = "#000000")
```
  
* Choosing 0.5 would contain most spam messages and avoid the second spike of HAM emails  
* We then evaluate this predictor  
```{r}
prediction <- ifelse(spam$your > 0.5, "spam", "nonspam")
res <- table(prediction, spam$type)/length(spam$type)
res
```
  
* In this case our accuracy is `r round(res[1,1], 4)` + `r round(res[2,2], 4)` = `r round(res[1,1]+res[2,2], 4)`, or an accuracy of approximately `r round((res[1,1]+res[2,2])*100, 2)`%, although this is an opptamistic measure of the overall error, which will be discussed further later.    


### Relative Importance of Steps  
question > data > features/variables > algorithms  
"*The commbinaiton of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.*" -John Tukey  

* In other words, an important component of prediction is knowing when to give up, that is that the data is not sufficient  

#### Input Data: Garbage in = Garbage out  
1. May be easy (movie ratings -> new movie ratings)  
2. May be harder (gene expression data -> disease)  
3. Depends on what is a "good prediction".  
4. Often more **[data > better models](http://www.youtube.com/watch?v=yvDCzhbjYWs)**  
5. The most important step is collecting the right data  

#### Features: They matter!  
* Properties of good features  
  + Lead to data compression  
  + Retain relevant information  
  + Are created based on expert application knowledge  
* Common mistakes  
  + Trying to automate feature selection (Although they may be automated with care)  
  + Not paying attention to data-specific quirks  
  + Throwing away information unnecessarily  

#### Algorithm: They Matter Less Than You'd Think  
![Linear vs Model](./Images/Linear_vs_Model.png)  
  
* The above table shows that the Linear Discrimenate Analysis (Lindisc) error often was not that far off from the best method  
* Using the best approach doesn't always largely improve the error  

#### Issues to Consider  
* The "Best" machine learning method would be:  
  + Interpretable  
    - If predictor is to be presented to an uninformed audience you'd want to to be understandable by them  
  + Simple  
    - Helps with interpretablity  
  + Accurate  
    - Getting a model to be interpretable can sometimes hurt the accuracy  
  + Fast  
    - Quick build the model, train, and test  
  + Scalable  
    - Easy to apply to a large dataset (either fast or parallelizable)  

#### Prediction is About Accuracy Tradeoffs  
* Tradeoffs are made for interpretability, speed, simplicity, or scalability.  
* Interpretability matters, decision tree-like results are more interpretable
  + "**if** total cholesterol $\geq 160$ **and** they smoke **then** *10 year CHD risk* $\geq$ *5%* **else if** they smoke **and** systolic blood pressure $\geq$ 140 **then** *10 year CHD risk* $\geq$ 5% **else** *10 year CHD risk* < 5%"  
* Scalability matter  
  + in "The Netflix $1 Million Challenge" Netflix never implemented the solution itself because the algorithm wasn't scalable and took way too long on the big data sets that Neflix was working with, so they went with something that was less accurate but more scalable.  


## Errors  
### In and Out of Sample Errors  
* **In Sample Error** - Sometimes called *resubstitution error*. The error rate you get on the same data set you used to build your predictor.  
* **Out of Sample Error** - Sometimes called *generalization error*. The error rate you get on a new data set.  
* Key Ideas  
1) Out of sample error is what you care about  
2) In sample error < out of sample error  
  + Sometimes you want to give up some accuracy on the data you have to have greater accuracy on unkown data.  
3) The reason is overfitting  
  + Matching your algorithm to the noise of the data you have  

#### Spam Example  
```{r warning = FALSE}
library(kernlab)
data(spam)
RNGkind(sample.kind = "Rounding")
set.seed(333)
smallSpam <- spam[sample(dim(spam)[1], size = 10),]
spamLabel <- (smallSpam$type == "spam")*1 + 1
plot(smallSpam$capitalAve, col = spamLabel, pch = 3)
abline(h = 2.7, col = "#FF0000")
abline(h = 2.40, col = "#000000")
abline(h = 2.80, col = "#5BC2E7")
```
  
##### Prediction 1  
* capitalAve > 2.7 = "spam"  
* capitalAve < 2.40 = "nonspam"
* We can add 2 params to make the prediction perfect for the training set  
  + capitalAve between 2.40 and 2.45 = "spam"  
  + capitalAve between 2.45 and 2.7 = "nonspam"  
```{r}
rule1 <- function(x){
  prediction <- rep(NA, length(x))
  prediction[x > 2.7] <- "spam"
  prediction[x < 2.40] <- "nonspam"
  prediction[(x >= 2.40 & x <= 2.45)] <- "spam"
  prediction[(x > 2.45 & x <= 2.70)] <- "nonspam"
  return(prediction)
}
table(rule1(smallSpam$capitalAve),smallSpam$type)
```

##### Prediction 2  
* (Note: The blue line in the plot is for 2.8)
* capitalAve > 2.80 = "spam"  
* capitalAve $\leq$ 2.80 = "nonspam"  
* This algorithm won't be perfect on the training data  
```{r}
rule2 <- function(x){
  prediction <- rep(NA, length(x))
  prediction[x > 2.8] <- "spam"
  prediction[x <= 2.8] <- "nonspam"
  return(prediction)
}
table(rule2(smallSpam$capitalAve), smallSpam$type)
```

##### Apply 2 rulesets to all spam data  
```{r}
table(rule1(spam$capitalAve),spam$type)
table(rule2(spam$capitalAve),spam$type)
paste0("Rule 1 accuracy: ", 
       mean(rule1(spam$capitalAve) == spam$type))
paste0("Rule 2 accuracy: ", 
       mean(rule2(spam$capitalAve) == spam$type))
paste0("Rule 1 total correct: ",
       sum(rule1(spam$capitalAve) == spam$type))
paste0("Rule 2 total correct: ",
       sum(rule2(spam$capitalAve) == spam$type))
```
  
#### Overfitting  
* Why is the ruleset with a *better* out of sample error (`rule2`) the one with a *worse* in sample error?  
  + It's because we overfitted `rule1` 
  + **[Wikipedia on Overfitting](http://en.wikipedia.org/wiki/Overfitting)**
* All data have two parts  
  + Signal - Part we are trying to use to predict  
  + Noise - Random variation in data set  
* The goal of a predictor is to find the signal and ignore the noise  
* You can always design a perfect in-sample predictor  
  + Doing this will capture botht he signal and the noise  
  + As such a predictor won't perform as well on new samples  
  
### Prediction Study Design  
1) Define your error rate  
2) Split data into:  
* Training set to build model  
* Testing set to validate model  
* Validation set (optional) to also validate the model  
3) On the training set pick features (using cross-validation to pick which features are most important in your model)  
4) On the training set pick prediction function (also using cross-validation)  
5a) If no validation set: 
* Apply the best model to the test set exactly 1 time  
  + If we apply multiple models to the test set and pick the best one we're kind of using the test set to train the model, giving an optimistic error rate  
5b) If there is a validation set: 
* Apply the model the the test set and refine the model  
* Then apply best model to validation set once  

#### Benchmarks  
* One should know what the prediction benchmarks are for their algorithm to help troubleshoot when something is going wrong. Often a benchmark is something like "all zeros" which tells the error rate if all values were set to 0, pretty much just ignore all the features of the dataset and taking a general average.  

#### Study Design of Netflix Contest  
![Netflix Design](./Images/studyDesign.png)
  
* Of all the data they split it into a training set (*Training Data*) that they shared with competators and a test & validation set (*Held Out Set*)that they did not share.  
* They shared a test set (*Probe*) so competators could test their out of sample error.  
* They would then take your model and test it on the *Quiz* set, although one could submit multiple models and get a scoring from the *Quiz* set. So they held out the last bit of data as a validation set (*Test*) that would be used at the end of the competition on each model only once.  

#### Avoid Small Sample Sizes  
* Suppose you are predicting a binary outcome  
  + Diseased/healthy  
  + (Not) Clicking on an ad  
* One classifier is flipping a coin  
* Probability of perfect classification is approximately $(\frac{1}{2})^{sample\space size}$  
  + $n = 1$ flipping coin 50% change of 100% accuracy  
  + $n = 2$ flipping coin 25% change of 100% accuracy  
  + $n = 10$ flipping coin 0.10% change of 100% accuracy  
* So lower sample sizes make it harder to know if your high accuracy is from chance or true.  

#### Rules of Thumb for Prediction Study Design  
* If you have a large sample size  
  + 60% training  
  + 20% test  
  + 20% validation  
* If you have a medium smaple size  
  + 60% training  
  + 40% testing  
* If you have a small sample size  
  + Do cross validation  
  + Reprot caveat of small sample size  

#### Some Principles to Remember  
* Set the test/validation set aside and *don't look at it*  
* In general *randomly* sample training and test sets  
* Your data sets must reflect structure of the problem  
  + If predictions evolve with time split train/test in time chunks (called backtesting in finance)  
* All subsets should reflect as much diversity as possible  
  + Random assignment does this  
  + You can also try to balance by features - but this is tricky  
  

### Types of Errors  
#### Basic Terms  
In general, **Positive** = identified and **Negative** = rejected. Whereas **True** and **False** indicate correctness. Therefore:  
* **True positive** = correctly identified  
* **False positive** = incorrectly identified  
* **True negative** = correctly rejected  
* **False negative** = incorrectly rejected  
  
*Medical testing example:*  
* **True positive** = Sick people correctly diagnosed as sick  
* **False positive** = Healthy people incorrectly identified as sick  
* **True negative** = Healthy people correctly identified as healthy  
* **False negative** = Sick people incorrectly identified as healthy  

* Sensitivity and Specificity  
  + **Sensitivity** - **True Positve Rate**, *Pr(positive test|disease)*, proportion of actual positives that are correctly identified  
  + **Specificity** - **True Negative Rate**, *Pr(negative test|no disease)* proportion of actual negatives that are correctly identified  
  + High amount of either ussually entails a high amount of the False ... Rate of the respective type, as ensuring you get all the positive/negative usually means you have to include some of the respective false samples.  
  + Sensitivity is likely to diagnois a positive (**Sen**tence the innocent)  
  + Specificity is going to be sure the positives are positive, even if they miss some (**Sp**are the innocent)  
* Other key quantities  
  + **Positive Predictive Value** - *Pr(disease|positive test)*  
  + **Negative Predictive Value** - *Pr(no disease | negative test)*  
  + **Accuracy** - *Pr(correct outcome)* = *Pr(positive test|disease) + Pr(negative test|no disease)*  

##### Key Quantities as Fractions  
```{r echo = FALSE}
tbl <- data.frame(Actually_Positive = c("TP", "FN"), 
           Actually_Negative = c("FP", "TN"))
row.names(tbl) <- c("Tested_Postive", "Tested_Negative")
tbl
```
  
* Sensitivity = $\frac{TP}{(TP+FN)}$  
* Specificity = $\frac{TN}{(FP+TN)}$  
* Positive Predictive Value = $\frac{TP}{(TP+FP)}$  
* Negative Predictive Value = $\frac{TN}{(FN+TN)}$  
* Accuracy = $\frac{(TP+TN)}{(TP+FP+FN+TN)}$  

#### Screening Tests Example  
Assume that some disease has a 0.1% prevalence in the population. Assume we have a test kit for that disease that works with 99% sensitivity and 99% specificity. What is the probability of a person having the disease given the test result is positive, if we randomly select a subject from:  
(We'll look at the expected values if we sampled 100000 people) 
* The general population?  
```{r echo=FALSE}
tbl[,1] <- c(99,1)
tbl[,2] <- c(999, 98901)
tbl
```
  
  + Sensitivity = $\frac{99}{(99+1)}$ = `r (99/(99+1))*100`%  
  + Specificity = $\frac{98901}{(999+98901)}$ = `r (98901/(999+98901))*100`%  
  + Positive Predictive Value = $\frac{99}{(99+999)}$ = `r round(((99)/(99+999))*100, 3)`%  
  + Negative Predictive Value = $\frac{98901}{(1+98901)}$ = `r round((98901/(1+98901))*100, 3)`%  
  + Accuracy = $\frac{(99+98901)}{100000}$ = `r ((99+98901)/100000)*100`%  

* A high risk sub-population with 10% disease prevalence  
```{r echo=FALSE}
tbl[,1] <- c(9900, 100)
tbl[,2] <- c(900, 89100)
tbl
```
  
  + Sensitivity = $\frac{9900}{(9900+100)}$ = `r (9900/(9900+100))*100`%  
  + Specificity = $\frac{89100}{(900+89100)}$ = `r (89100/(900+89100))*100`%  
  + Positive Predictive Value = $\frac{9900}{(9900+900)}$ = `r round(((9900)/(9900+900))*100, 3)`%  
  + Negative Predictive Value = $\frac{89100}{(100+89100)}$ = `r round((89100/(100+89100))*100, 3)`%  
  + Accuracy = $\frac{(9900+89100)}{100000}$ = `r ((9900+89100)/100000)*100`%  
 
* This low Postitive Predictive Value shows the issues with predicting a very rare event from a population versus something that's more prevalent.  

#### For Continous Data  
We evaluate error by *mean squared error* and it's root  
* **Mean squared error (MSE)** - $\frac{1}{n}\sum_{i=1}^n(Prediction_i-Truth_i)^2$  
* **Root mean squared error (RMSE)** - $\sqrt{\frac{1}{n}\sum_{i=1}^n(Prediction_i-Truth_i)^2}$  

#### Common Error Measures  
1. Mean squared error (or root mean squared error)  
* Continous data, sensitive to outliers  
2. Median absolute deviation  
* Median of distance between predicted and observed and take absolute value, rather than squared distance (this requries all values to be positive).  
* Continous data, often more robust  
3. Sensitivity (recall)  
* If you want few missed positives  
4. Specificity  
* If you want few negatives called positives  
5. Accuracy  
* Weights false positives/negatives equally  
6. Concordance  
* An example is **[kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa)**  
7. Predictive value of a positive (precision)  
* When you are screening and prevalence is low  


### Receiver Operating Characteristics (ROC Curves)   
* Used to meaure the quality of a prediction algorithm  
* **[Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)**
* Predictions are often quantitative  
  + Proabbility fo being alive  
  + Prediction on a scale from 1 to 10  
* The *cutoff* you choose gives different results  
* The curve informs you of the tradeoff of giving up some specificity for sensitivity (or vice versa)  
* The curves plot the $P(FP)$ (x-axis) versus $P(TP)$ (y-axis)  
![Example of a ROC Curve](./Images/ROCexample.png)  
  
#### Area Under the Curve  
* The area under the curve (AKA the integral) describes the effectiveness of a given algorithm  
* AUC = 0.5: random guessing  
* AUC = 1: perfect classifier (given a certain value of the perdiction algorithm)  
  + As such the closer to the top left of the plot a curve is the better it is.  
* In general (depending on field & probability) an AUC above 0.8 is considered "good"  

**Reminder to Commit (03), Delete this line** ***AFTER*** **Committing**  

## Cross Validation  
### Cross Validation  
### What Data Should You Use?  

**Reminder to Commit (04), Delete this line** ***AFTER*** **Committing**  

## Quiz 1  

**Reminder to Commit (Q1), Delete this line** ***AFTER*** **Committing**  

# The Caret Package  
## Caret Package  
### Caret Package  
### Training Options  
### Plotting Predictors  

**Reminder to Commit (05), Delete this line** ***AFTER*** **Committing**  

## Preprocessing  
### Basic Preprocessing  
### Covariate Creation  
### Preprocessing with Principal Components Analysis (PCA)  

**Reminder to Commit (06), Delete this line** ***AFTER*** **Committing**  

## Predicting  
### Predicting with Regression  
### Predicting with Regression Multiple Covariates  

**Reminder to Commit (07), Delete this line** ***AFTER*** **Committing**  

## Quiz 2  

**Reminder to Commit (Q2), Delete this line** ***AFTER*** **Committing**  

# Predicting with Trees, Random Forests, & Model Based Predictions  
## Trees  
### Predicting with Trees  
### Bagging  

**Reminder to Commit (08), Delete this line** ***AFTER*** **Committing**  

## Random Forests  
### Random Forests  
### Boosting  

**Reminder to Commit (09), Delete this line** ***AFTER*** **Committing**  

## Model Baded Predictions  
### Model Based Predictions  

**Reminder to Commit (10), Delete this line** ***AFTER*** **Committing**  

## Quiz 3  

**Reminder to Commit (Q3), Delete this line** ***AFTER*** **Committing**  

# Regularized Regression and Combining Predictors  
## Regularized Regression  
## Combining Predictors  

**Reminder to Commit (11), Delete this line** ***AFTER*** **Committing**  

## Forecasting  
## Unsupervised Prediction  

**Reminder to Commit (12), Delete this line** ***AFTER*** **Committing**  

## Quiz 4  

**Reminder to Commit (Q4), Delete this line** ***AFTER*** **Committing**  

# Course Project  

**Reminder to Commit (P1), Delete this line** ***BEFORE*** **Committing**  

---
title: "Practical Machine Learning Notes"
author: "Coursera Course by John Hopkins University"
date: "INSTRUCTORS: Dr. Jeff Leek, Dr. Brian Caffo, Dr. Roger D. Peng"
fontsize: 11pt
output: 
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro  
* This course covers the basic ideas behind machine learning/prediction  
  + Study Design - training vs. test sets  
  + Conceptual issues - out of sample error, overfitting, ROC curves  
  + Practical Implementation - the caret package  
* What this course depends on:  
  + The Data Scientist's Toolbox  
  + R Programming  
* What would be useful  
  + Exploratory Analysis  
  + Reproducible Research  
  + Regression Models  
  + (Notes on these 5 courses are all in my GitHub repoes)  
  
## GitHub Link for Lectures  
**[Practical Machine Learning lectures on GitHub](https://github.com/bcaffo/courses/tree/master/08_PracticalMachineLearning)**  

## Course Book  
**[The book for this course is available on this site](https://web.stanford.edu/~hastie/ElemStatLearn//)**  

## Instructor's Note  
"*Welcome to Practical Machine Learning! This course will focus on developing the tools and techniques for understanding, building, and testing prediction functions.*  
  
*These tools are at the center of the Data Science revolution. Many researchers, companies, and governmental organizations would like to use the cheap and abundant data they are collecting to predict what customers will like, what services to offer, or how to improve people's lives.*  
  
*Jeff Leek and the Data Science Track Team*"  


# Prediction, Errors, and Cross Validation  
## Prediction  
### Prediction Motivation  
* Who predicts things?  
  + Local governments -> pension payments  
  + Google -> whether you will click on an ad  
  + Amazon -> what movies you will watch  
  + Insurance companies -> what your risk of death is  
  + Johns Hopkins -> who will succeed in their programs  
* Why predict things  
  + Glory (Nerd cred for accomplishing certain feats)  
    - A lot of competitions are hosted on **[Kaggle](http://www.kaggle.com/)**  
  + Riches (Completing some competition that offers a reward)  
  + Save lives  
    - **[On cotype DX](http://www.oncotypedx.com/en-US/Home)** reveals the underlying biology that changes treatment decisions 37% of the time.  

#### More Resources
* **[A course on more advanced material about ML](https://www.coursera.org/course/ml)**  
* **[List of machine learning resources on Quora](http://www.quora.com/Machine-Learning/What-are-some-good-resources-for-learning-about-machine-learning-Why)**  
* **[List of machine learning resources from Science](http://www.sciencemag.org/site/feature/data/compsci/machine_learning.xhtml)**  
* **[Advanced notes from MIT open courseware](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)**  
* **[Advanced notes from CMU](http://www.stat.cmu.edu/~cshalizi/350/)**  
* **[Kaggle - machine learning competitions](http://www.kaggle.com/)**  

### What is Prediction?  
#### Main Idea  
* One focus of ML is on what algorithms are the best for extracting information and using it to predict.  
* Although the method used for producing a training set is also quite important  
![Main Idea of ML](./Images/ML_Main_Idea.png)
  
* One starts off with a dataset  
1) One uses Probability/Sampling to select a Training Set  
2) One measures characteristics of this training set to create a Predicition Function  
3) One then uses the Prediction Function to take an uncolored dot and predict if it's red or blue  
4) One would then go on to test how well their Prediction Function works  

#### What Can Go Wrong  
* An example is **[Google Flu trends](http://www.sciencemag.org/content/343/6176/1203)** (**[A free overview of the issue witht he accuracy](https://en.wikipedia.org/wiki/Google_Flu_Trends#Accuracy)**)  
  - Google tried to predict rate of flu using what people would search  
  - Originally the algorithm was able to represent how many cases would appear in a region within a certain time  
  - Although they didn't account for the fact that the terms would change over time  
  - The way the terms were being used wasn't well understood so when terms changed they weren't able to accurately account for the change.  
  - It also overestimated as it the search terms it looked at were often cofactors with other illnesses  
  
#### Componets of a Predictor  
1) Question  
* Any problem in data science starts with a question, "What are you trying to predict and what are you trying to predict it with?"  

2) Input Data  
* Collect best input data you can to use to predict  

3) Features  
* From that data one builds features that they will use to predict  

4) Algorithm  
* One uses ML algorithms to develop a function  

5) Parameters  
* Estimate parameters of the algorithm  

6) Evaluation  
* Apply algorithm to a data set to evaluate how well the algorithm works  

#### Example  
* Start with a general question, "Can I automatically detect emails that are SPAM and those that are not?"  
* Make the question more concrete, "Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?"  
* Find input data  
  + In this instance there is data avaliable in R via the `kernlab` package  
  + Note that this data set won't necessarily be the perfect data as it doesn't contain all the emails ever sent, or the emails sent to you personally  
* Quantify features, such as the frequency of certain words or typeface. The `spam` dataset from `kernlab` contains these types of frequency.  
```{r}
library(kernlab)
data(spam)
str(spam)
```
```{r}
plot(density(spam$your[spam$type=="nonspam"]),
     col = "#5BC2E7", main = "", xlab = "Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]), col = "#FF0000")
```
  
 * It can be seen here "your" appears more often in SPAM emails than it does in HAM  
  + One could use this idea to create a cut-off point for predicting if a message is SPAM  
* The proposed algorithm  
  + Find a value of $C$  
  + If the frequency of $'your'>C$ predict the message is SPAM  
```{r}
plot(density(spam$your[spam$type=="nonspam"]),
     col = "#5BC2E7", main = "", xlab = "Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]), col = "#FF0000")
abline(v = 0.5, col = "#000000")
```
  
* Choosing 0.5 would contain most spam messages and avoid the second spike of HAM emails  
* We then evaluate this predictor  
```{r}
prediction <- ifelse(spam$your > 0.5, "spam", "nonspam")
res <- table(prediction, spam$type)/length(spam$type)
res
```
  
* In this case our accuracy is `r round(res[1,1], 4)` + `r round(res[2,2], 4)` = `r round(res[1,1]+res[2,2], 4)`, or an accuracy of approximately `r round((res[1,1]+res[2,2])*100, 2)`%, although this is an opptamistic measure of the overall error, which will be discussed further later.    


### Relative Importance of Steps  
question > data > features/variables > algorithms  
"*The commbinaiton of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.*" -John Tukey  

* In other words, an important component of prediction is knowing when to give up, that is that the data is not sufficient  

#### Input Data: Garbage in = Garbage out  
1. May be easy (movie ratings -> new movie ratings)  
2. May be harder (gene expression data -> disease)  
3. Depends on what is a "good prediction".  
4. Often more **[data > better models](http://www.youtube.com/watch?v=yvDCzhbjYWs)**  
5. The most important step is collecting the right data  

#### Features: They matter!  
* Properties of good features  
  + Lead to data compression  
  + Retain relevant information  
  + Are created based on expert application knowledge  
* Common mistakes  
  + Trying to automate feature selection (Although they may be automated with care)  
  + Not paying attention to data-specific quirks  
  + Throwing away information unnecessarily  

#### Algorithm: They Matter Less Than You'd Think  
![Linear vs Model](./Images/Linear_vs_Model.png)  
  
* The above table shows that the Linear Discrimenate Analysis (Lindisc) error often was not that far off from the best method  
* Using the best approach doesn't always largely improve the error  

#### Issues to Consider  
* The "Best" machine learning method would be:  
  + Interpretable  
    - If predictor is to be presented to an uninformed audience you'd want to to be understandable by them  
  + Simple  
    - Helps with interpretablity  
  + Accurate  
    - Getting a model to be interpretable can sometimes hurt the accuracy  
  + Fast  
    - Quick build the model, train, and test  
  + Scalable  
    - Easy to apply to a large dataset (either fast or parallelizable)  

#### Prediction is About Accuracy Tradeoffs  
* Tradeoffs are made for interpretability, speed, simplicity, or scalability.  
* Interpretability matters, decision tree-like results are more interpretable
  + "**if** total cholesterol $\geq 160$ **and** they smoke **then** *10 year CHD risk* $\geq$ *5%* **else if** they smoke **and** systolic blood pressure $\geq$ 140 **then** *10 year CHD risk* $\geq$ 5% **else** *10 year CHD risk* < 5%"  
* Scalability matter  
  + in "The Netflix $1 Million Challenge" Netflix never implemented the solution itself because the algorithm wasn't scalable and took way too long on the big data sets that Neflix was working with, so they went with something that was less accurate but more scalable.  


## Errors  
### In and Out of Sample Errors  
* **In Sample Error** - Sometimes called *resubstitution error*. The error rate you get on the same data set you used to build your predictor.  
* **Out of Sample Error** - Sometimes called *generalization error*. The error rate you get on a new data set.  
* Key Ideas  
1) Out of sample error is what you care about  
2) In sample error < out of sample error  
  + Sometimes you want to give up some accuracy on the data you have to have greater accuracy on unkown data.  
3) The reason is overfitting  
  + Matching your algorithm to the noise of the data you have  

#### Spam Example  
```{r warning = FALSE}
library(kernlab)
data(spam)
RNGkind(sample.kind = "Rounding")
set.seed(333)
smallSpam <- spam[sample(dim(spam)[1], size = 10),]
spamLabel <- (smallSpam$type == "spam")*1 + 1
plot(smallSpam$capitalAve, col = spamLabel, pch = 3)
abline(h = 2.7, col = "#FF0000")
abline(h = 2.40, col = "#000000")
abline(h = 2.80, col = "#5BC2E7")
```
  
##### Prediction 1  
* capitalAve > 2.7 = "spam"  
* capitalAve < 2.40 = "nonspam"
* We can add 2 params to make the prediction perfect for the training set  
  + capitalAve between 2.40 and 2.45 = "spam"  
  + capitalAve between 2.45 and 2.7 = "nonspam"  
```{r}
rule1 <- function(x){
  prediction <- rep(NA, length(x))
  prediction[x > 2.7] <- "spam"
  prediction[x < 2.40] <- "nonspam"
  prediction[(x >= 2.40 & x <= 2.45)] <- "spam"
  prediction[(x > 2.45 & x <= 2.70)] <- "nonspam"
  return(prediction)
}
table(rule1(smallSpam$capitalAve),smallSpam$type)
```

##### Prediction 2  
* (Note: The blue line in the plot is for 2.8)
* capitalAve > 2.80 = "spam"  
* capitalAve $\leq$ 2.80 = "nonspam"  
* This algorithm won't be perfect on the training data  
```{r}
rule2 <- function(x){
  prediction <- rep(NA, length(x))
  prediction[x > 2.8] <- "spam"
  prediction[x <= 2.8] <- "nonspam"
  return(prediction)
}
table(rule2(smallSpam$capitalAve), smallSpam$type)
```

##### Apply 2 rulesets to all spam data  
```{r}
table(rule1(spam$capitalAve),spam$type)
table(rule2(spam$capitalAve),spam$type)
paste0("Rule 1 accuracy: ", 
       mean(rule1(spam$capitalAve) == spam$type))
paste0("Rule 2 accuracy: ", 
       mean(rule2(spam$capitalAve) == spam$type))
paste0("Rule 1 total correct: ",
       sum(rule1(spam$capitalAve) == spam$type))
paste0("Rule 2 total correct: ",
       sum(rule2(spam$capitalAve) == spam$type))
```
  
#### Overfitting  
* Why is the ruleset with a *better* out of sample error (`rule2`) the one with a *worse* in sample error?  
  + It's because we overfitted `rule1` 
  + **[Wikipedia on Overfitting](http://en.wikipedia.org/wiki/Overfitting)**
* All data have two parts  
  + Signal - Part we are trying to use to predict  
  + Noise - Random variation in data set  
* The goal of a predictor is to find the signal and ignore the noise  
* You can always design a perfect in-sample predictor  
  + Doing this will capture botht he signal and the noise  
  + As such a predictor won't perform as well on new samples  
  
### Prediction Study Design  
1) Define your error rate  
2) Split data into:  
* Training set to build model  
* Testing set to validate model  
* Validation set (optional) to also validate the model  
3) On the training set pick features (using cross-validation to pick which features are most important in your model)  
4) On the training set pick prediction function (also using cross-validation)  
5a) If no validation set: 
* Apply the best model to the test set exactly 1 time  
  + If we apply multiple models to the test set and pick the best one we're kind of using the test set to train the model, giving an optimistic error rate  
5b) If there is a validation set: 
* Apply the model the the test set and refine the model  
* Then apply best model to validation set once  

#### Benchmarks  
* One should know what the prediction benchmarks are for their algorithm to help troubleshoot when something is going wrong. Often a benchmark is something like "all zeros" which tells the error rate if all values were set to 0, pretty much just ignore all the features of the dataset and taking a general average.  

#### Study Design of Netflix Contest  
![Netflix Design](./Images/studyDesign.png)
  
* Of all the data they split it into a training set (*Training Data*) that they shared with competators and a test & validation set (*Held Out Set*)that they did not share.  
* They shared a test set (*Probe*) so competators could test their out of sample error.  
* They would then take your model and test it on the *Quiz* set, although one could submit multiple models and get a scoring from the *Quiz* set. So they held out the last bit of data as a validation set (*Test*) that would be used at the end of the competition on each model only once.  

#### Avoid Small Sample Sizes  
* Suppose you are predicting a binary outcome  
  + Diseased/healthy  
  + (Not) Clicking on an ad  
* One classifier is flipping a coin  
* Probability of perfect classification is approximately $(\frac{1}{2})^{sample\space size}$  
  + $n = 1$ flipping coin 50% change of 100% accuracy  
  + $n = 2$ flipping coin 25% change of 100% accuracy  
  + $n = 10$ flipping coin 0.10% change of 100% accuracy  
* So lower sample sizes make it harder to know if your high accuracy is from chance or true.  

#### Rules of Thumb for Prediction Study Design  
* If you have a large sample size  
  + 60% training  
  + 20% test  
  + 20% validation  
* If you have a medium smaple size  
  + 60% training  
  + 40% testing  
* If you have a small sample size  
  + Do cross validation  
  + Reprot caveat of small sample size  

#### Some Principles to Remember  
* Set the test/validation set aside and *don't look at it*  
* In general *randomly* sample training and test sets  
* Your data sets must reflect structure of the problem  
  + If predictions evolve with time split train/test in time chunks (called backtesting in finance)  
* All subsets should reflect as much diversity as possible  
  + Random assignment does this  
  + You can also try to balance by features - but this is tricky  
  

### Types of Errors  
#### Basic Terms  
In general, **Positive** = identified and **Negative** = rejected. Whereas **True** and **False** indicate correctness. Therefore:  
* **True positive** = correctly identified  
* **False positive** = incorrectly identified  
* **True negative** = correctly rejected  
* **False negative** = incorrectly rejected  
  
*Medical testing example:*  
* **True positive** = Sick people correctly diagnosed as sick  
* **False positive** = Healthy people incorrectly identified as sick  
* **True negative** = Healthy people correctly identified as healthy  
* **False negative** = Sick people incorrectly identified as healthy  

* Sensitivity and Specificity  
  + **Sensitivity** - **True Positve Rate**, *Pr(positive test|disease)*, proportion of actual positives that are correctly identified  
  + **Specificity** - **True Negative Rate**, *Pr(negative test|no disease)* proportion of actual negatives that are correctly identified  
  + High amount of either ussually entails a high amount of the False ... Rate of the respective type, as ensuring you get all the positive/negative usually means you have to include some of the respective false samples.  
  + Sensitivity is likely to diagnois a positive (**Sen**tence the innocent)  
  + Specificity is going to be sure the positives are positive, even if they miss some (**Sp**are the innocent)  
* Other key quantities  
  + **Positive Predictive Value** - *Pr(disease|positive test)*  
  + **Negative Predictive Value** - *Pr(no disease | negative test)*  
  + **Accuracy** - *Pr(correct outcome)* = *Pr(positive test|disease) + Pr(negative test|no disease)*  

##### Key Quantities as Fractions  
```{r echo = FALSE}
tbl <- data.frame(Actually_Positive = c("TP", "FN"), 
           Actually_Negative = c("FP", "TN"))
row.names(tbl) <- c("Tested_Postive", "Tested_Negative")
tbl
```
  
* Sensitivity = $\frac{TP}{(TP+FN)}$  
* Specificity = $\frac{TN}{(FP+TN)}$  
* Positive Predictive Value = $\frac{TP}{(TP+FP)}$  
* Negative Predictive Value = $\frac{TN}{(FN+TN)}$  
* Accuracy = $\frac{(TP+TN)}{(TP+FP+FN+TN)}$  

#### Screening Tests Example  
Assume that some disease has a 0.1% prevalence in the population. Assume we have a test kit for that disease that works with 99% sensitivity and 99% specificity. What is the probability of a person having the disease given the test result is positive, if we randomly select a subject from:  
(We'll look at the expected values if we sampled 100000 people) 
* The general population?  
```{r echo=FALSE}
tbl[,1] <- c(99,1)
tbl[,2] <- c(999, 98901)
tbl
```
  
  + Sensitivity = $\frac{99}{(99+1)}$ = `r (99/(99+1))*100`%  
  + Specificity = $\frac{98901}{(999+98901)}$ = `r (98901/(999+98901))*100`%  
  + Positive Predictive Value = $\frac{99}{(99+999)}$ = `r round(((99)/(99+999))*100, 3)`%  
  + Negative Predictive Value = $\frac{98901}{(1+98901)}$ = `r round((98901/(1+98901))*100, 3)`%  
  + Accuracy = $\frac{(99+98901)}{100000}$ = `r ((99+98901)/100000)*100`%  

* A high risk sub-population with 10% disease prevalence  
```{r echo=FALSE}
tbl[,1] <- c(9900, 100)
tbl[,2] <- c(900, 89100)
tbl
```
  
  + Sensitivity = $\frac{9900}{(9900+100)}$ = `r (9900/(9900+100))*100`%  
  + Specificity = $\frac{89100}{(900+89100)}$ = `r (89100/(900+89100))*100`%  
  + Positive Predictive Value = $\frac{9900}{(9900+900)}$ = `r round(((9900)/(9900+900))*100, 3)`%  
  + Negative Predictive Value = $\frac{89100}{(100+89100)}$ = `r round((89100/(100+89100))*100, 3)`%  
  + Accuracy = $\frac{(9900+89100)}{100000}$ = `r ((9900+89100)/100000)*100`%  
 
* This low Postitive Predictive Value shows the issues with predicting a very rare event from a population versus something that's more prevalent.  

#### For Continous Data  
We evaluate error by *mean squared error* and it's root  
* **Mean squared error (MSE)** - $\frac{1}{n}\sum_{i=1}^n(Prediction_i-Truth_i)^2$  
* **Root mean squared error (RMSE)** - $\sqrt{\frac{1}{n}\sum_{i=1}^n(Prediction_i-Truth_i)^2}$  

#### Common Error Measures  
1. Mean squared error (or root mean squared error)  
* Continous data, sensitive to outliers  
2. Median absolute deviation  
* Median of distance between predicted and observed and take absolute value, rather than squared distance (this requries all values to be positive).  
* Continous data, often more robust  
3. Sensitivity (recall)  
* If you want few missed positives  
4. Specificity  
* If you want few negatives called positives  
5. Accuracy  
* Weights false positives/negatives equally  
6. Concordance  
* An example is **[kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa)**  
7. Predictive value of a positive (precision)  
* When you are screening and prevalence is low  


### Receiver Operating Characteristics (ROC Curves)   
* Used to meaure the quality of a prediction algorithm  
* **[Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)**
* Predictions are often quantitative  
  + Proabbility fo being alive  
  + Prediction on a scale from 1 to 10  
* The *cutoff* you choose gives different results  
* The curve informs you of the tradeoff of giving up some specificity for sensitivity (or vice versa)  
* The curves plot the $P(FP)$ (x-axis) versus $P(TP)$ (y-axis)  
![Example of a ROC Curve](./Images/ROCexample.png)  
  
#### Area Under the Curve  
* The area under the curve (AKA the integral) describes the effectiveness of a given algorithm  
* AUC = 0.5: random guessing  
* AUC = 1: perfect classifier (given a certain value of the perdiction algorithm)  
  + As such the closer to the top left of the plot a curve is the better it is.  
* In general (depending on field & probability) an AUC above 0.8 is considered "good"  

## Cross Validation  
### Cross Validation  
* A widely used tool for detecting relevant features and building models.  

#### Key Ideas  
1. Accuracy on the training set (resubstitution accuracy) is optimistic  
2. A better estimate comes from an independent set (test set accuracy)  
3. But we can't use the test set when building the model or it becomems part of the training set  
4. So we estimate the test set accuracy with the training set  

Cross-Validation Approach:  
1. Use the training set  
2. Split it into training/test sets (seperate from actual test set)  
3. Build a model on the training set  
4. Evaluate on the test set  
5. Repeat with new training/test sets and average the estimated errors  
  
What Cross-Validation is used for:  
1. Picking variables to include in a model  
2. Picking th etype of prediction function to use  
3. Picking the parameters in the prediction function  
4. Comparing different predictors  
  
#### Ways to Pick Subsets  
![Random Subsampling](./Images/randomSubsampling.png)
  
![K-folds](./Images/kfold.png)  
  
* Breaks data into $K$ equal sized data sets.  

![Leave One Out](./Images/LeaveOneOut.png)  
  
* Leave out 1 sample then train on all the others, repeat for all samples  

#### Considerations  
* For time series data, data must be used in "chunks"  
* For k-fold cross validation  
  + Larger k = less bias, more variance  
  + Smaller k = more bias, less variance  
* Random sampling must be done *without replacement*  
* Random sampling with replacement is the *bootstrap*  
  + Underestimates of the error  
  + Can be corrected, but it is complicated (**[0.632 Bootstrap](https://www.jstor.org/stable/2965703?seq=1)**)  
* If you cross-validate to pick predictors estimate you must estiamte errors on independent data.  



### What Data Should You Use?  
* If you want to predict something about *X* use data related to *X* (Use like to predict like)  
  + To predict player performance use data about player performance  
  + To predict movie preferences use data about movie preferences  
  + To predict hospitalizations sue data about hospitalizations  
* The closer the data is to the process you want to predict the better the predictions will be  
* The looser connection the harder the prediction may be  
  + *On Cotype DX* uses gene expression to predict one's longevity and effectiveness of treatments for those with breast cancer.  
* Unrelated data is the most common mistake, **[for example:](http://www.nejm.org/doi/full/10.1056/NEJMon1211064)**  
![Chocolate Consumption vs. Nobel Prizes](./Images/choc.png)
  
  + There are many alternate variables one could look at that are more realisticly correlated  

## Quiz 1  
(Note 1-4 were multiple choice of material covered in the notes)  

5. Suppose that we have created a machine learning algorithm that predicts whether a link will be clicked with 99% sensitivity and 99% specificity. The rate the link is clicked is 1/1000 of visits to a website. If we predict the link will be clicked on a specific visit, what is the probability it will actually be clicked?

```{r}
sens <- 0.99  #TP/(TP+FN)
spec <- 0.99  #TN/(FP+TN)
rate <- 1/1000 #sum(TP+FN), 1-r is sum(FP+TN)  
#ppv = TP/(TP + FP)

#rate = TP+FN,
#rate - TP = FN
#sens * (TP+FN) = TP,
#sens * (TP + rate - TP) = TP,
#TP = sens * rate
TP <- sens * rate

#1 - rate = FP + TN,
#1 - rate - FP = TN

#spec * (FP+TN) = TN,
#spec * (FP+1 - rate - FP) = 1 - rate - FP
#spec * (1 - rate) = 1 - rate - FP
# FP = 1 - rate - (spec* (1-rate))
# FP = (1-rate)*(1-spec)
FP <- (1-rate)*(1-spec)
ppv <- TP/(TP + FP)
ppv
```


# The Caret Package  
## Caret Package  
### Caret Package  
* Can be installed with `install.packages("caret")`, details about the package **[can be found on cran](https://cran.r-project.org/web/packages/caret/index.html)**  

#### Functionality  
* Some preprocessing (cleaning)  
  + `preProcess`  
* Data splitting  
  + `createDataPartition`  
  + `createResample`  
  + `createTimeSlices`  
* Training/testing functions  
  + `train`  
  + `predict`  
* Model comparison  
  + `confusionMatrix`  
  
#### Machine Learning Algorithms in Base R  
* Linear discriminant analysis  
* Regression  
* Naive Bayes  
* Support vector machines  
* Classification and regression trees  
* Random forests  
* Boosting  
* etc.  
* The interface for these algorithms is slightly different  
![predict function parameters in base R](./Images/base_r_predict_params.png)  
  
* The `caret` package unifies these differences  

#### SPAM Example: Data Splitting  
```{r}
library(caret)
library(kernlab) #For data 
data(spam)
set.seed(32343)
inTrain <- createDataPartition(y = spam$type,
                               p = 0.75, #Proportion to subset
                               list = FALSE) # =F returns indecies 
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
dim(spam)
dim(training)
dim(spam)[1]*0.75 #Showing it took 75%
```

```{r warning = FALSE}
set.seed(32343)
modelFit <- train(type ~ ., data = training, method = "glm")
modelFit
```
  
* `Resampling: Bootstrapped (25 reps)` indicates that it used the Bootstrap method, with 25 replicates. It corrects for the error that can occur when using the bootstrap method  
```{r}
modelFit$finalModel
```
  
* This shows all how all the variables are weighted  

#### SPAM Example: Prediction  
```{r}
predictions <- predict(modelFit, newdata = testing)
predictions[1:30]
```

#### SPAM Example: Confusion Matrix  
```{r}
confusionMatrix(predictions, testing$type)
```

* First gives a table of the predicted vs. true value  
* Gives summary statistics  
  + Accuracy & 95% CI for the accuracy  
  + No Information Rate is the average loss, $L$, of $f$ over all combinations of $y_i$ and $x_j$, expressed with the formula: $\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^nL(y_i,f(x_j))$  
  + The **[Mcnemar's Test P-Value](https://en.wikipedia.org/wiki/McNemar%27s_test)** has a null hypothesis that the error rates are equivelant  
  
#### Further Information  
* Caret tutorials:
  + **[PDF caret tutorial](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)**
  + **[cran vignette PDF ](http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)**
  + **[A paper introducing the caret package](http://www.jstatsoft.org/v28/i05/paper)**

### Data Slicing  
#### SPAM Example: Data Splitting  
```{r}
library(caret)
library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, 
                               p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
```

#### SPAM Example: K-fold  
```{r}
set.seed(32323)
folds <- createFolds(y=spam$type, k = 10,
                    list = TRUE, returnTrain = TRUE) #Returns sample positions
sapply(folds, length)
folds[[1]][1:10]

#You can also have it return the test set
set.seed(32323)
folds <- createFolds(y=spam$type, k = 10,
                    list = TRUE, returnTrain = FALSE) #Returns sample positions
sapply(folds, length)
folds[[1]][1:10]

```

#### SPAM Example: Resampling  
```{r}
set.seed(32323)
folds <- createResample(y = spam$type, times = 10,
                        list = TRUE)
sapply(folds, length)
folds[[1]][1:10] #Contains some resampled values
```

#### SPAM Example: Time Slices  
```{r}
set.seed(32323)
tme <- 1:1000
folds <- createTimeSlices(y = tme, 
                          initialWindow = 20, #consecutive ... training set 
                          horizon = 10)#Consecutive values in each test set
names(folds)
folds$train[[1]]
folds$test[[1]]
```


### Training Options  
```{r warning=FALSE}
## Still using SPAM set  
library(caret)
library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
modelFit <- train(type ~., data = training, method = "glm")
```

#### Train Options  
```{r}
args(caret:::train.default)
```
  
* One can change ...  
  + `preProcess` to change preprocessing options (covered later)  
  + `weights` to assign weights to the variables (Useful for unbalanced training set)  
  + `metric` to change what is measured, default is that Accuracy is measured for catagorigal variables and RMSE otherwise. Below are some of the other options:  
    - RMSE = Root mean squared error  
    - RSquared = $R^2$ from regression models  
    - Accuracy = Fraction correct  
    - Kappa = A measure of **[concordance](http://en.wikipedia.org/wiki/Cohen%27s_kappa)**    
    
  + `trControl` = Calls to `trainControl` function which has more of it's own settings:  
```{r}
args(trainControl)
```
    - `method` will determine how it samples data, along with the `number` of times and how many the process `repeats`.  
    - `initialWindow` and `horizon` is for time based data  
    - `savePredictions` if true will return all the predictors of each model  
    - `summaryFunction` will determine the kind of summary returned  
    - `preProcOptions` (Preprocessing options)  
    - `seeds` is available to set seeds for all the diffrent resampling layers, helpful when running in parallel. Often useful to set an overall seed outside this argument.
    
#### trainControl Resampling  
* `method`  
  + *boot* = bootstrapping  
  + *boot632* = bootstrapping with adjustment  
  + *cv* = cross validation  
  + *repeatedcv* = repeated cross validation  
  + *LOOCV* = leave one out cross validation  
* `number`  
  + For boot/cross validation  
  + Number of subsamples to take  
* `repeats`  
  + Number of times to repeat subsampling  
  + If value is big this can slow things down  
  
* **[More info on model training and tuning](http://caret.r-forge.r-project.org/training.html)**  


### Plotting Predictors  
```{r message = FALSE}
#The example in this lesson will be using wages data from the ISLR package  
library(ISLR); data(Wage)
library(tidyverse)
library(caret)
summary(Wage)
```
  
* From this we can see the data is all from Males in the Middle Atlantic region  
```{r}
## Creating training/test sets  
set.seed(1618033)
inTrain <- createDataPartition(y = Wage$wage,
                               p = 0.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
dim(training); dim(testing)
```

#### Looking at the Data  
##### featurePlot  
```{r}
featurePlot(x = training[, c("age", "education", "jobclass")],
            y = training$wage,
            plot = "pairs")
```

* When looking at this plot one is looking for trends in the data, for example the second column of the first row shows a semi-positive correlation to the X, when deciphering the mess of words one can see this is our y, `wage`, against `education`; indicating higher education might correlate to a higher wage.  
* Two catagorical variables against eachother are hard to decipher meaning from as they largely overlap.  

##### qplot  
* Quick plots in style of `ggplot`  
```{r}
qplot(age, wage, data = training)
```

* The odd subset of wages that are away from the others may be cause for some concern, as such we'd want to investigate this before making the model  
```{r}
qplot(age, wage, colour = jobclass, data = training)
```

* You can also add regression smoothers to investigate differences more  
```{r}
plot <- qplot(age, wage, colour = education, data = training)
plot + geom_smooth(method = 'lm', formula = y ~ x)
```

##### Using cut2 to make factors  
```{r warning = FALSE}
library(Hmisc)
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
```
```{r}
bplot1 <- qplot(cutWage, age, data = training, fill = cutWage,
               geom = c("boxplot"))
bplot1
```

```{r}
#Add points overlayed on boxplot
bplot2 <- qplot(cutWage, age, data = training, fill = cutWage, 
                geom = c("boxplot", "jitter"))
bplot2
```

* Many points for each plot indicates the boxplots are well representing the data, if there were only a few then it would suggest the boxplots are not as representative  

##### Tables  
```{r}
t1 <- table(cutWage, training$jobclass)
t1
prop.table(t1, 1) #Shows proportions, 1 for by row
```

##### Density Plots  
```{r}
qplot(wage, colour = education, data = training, geom = "density")
```

#### Notes and Further Reading  
* Make your plots only in the training set  
  + Don't use the test set for exploration!  
* Things one should be looking for  
  + Imbalance in outcomes/predictors  
  + Outliers  
  + Groups of points not explained by a predictor  
  + Skewed variables  
* **[ggplot2 tutorial](http://rstudio-pubs-static.s3.amazonaws.com/2176_75884214fc524dc0bc2a140573da38bb.html)**  
* **[caret visualizations](http://caret.r-forge.r-project.org/visualizations.html)**  


## Preprocessing  
### Basic Preprocessing  
* Sometimes variables need to be transformed to be more helpful for prediction algorithms  

#### Why Preprocess? : Looking at SPAM Example  
```{r}
library(kernlab); data(spam)
library(caret)
set.seed(1618033)
inTrain <- createDataPartition(y = spam$type,
                               p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
hist(training$capitalAve, main = "", xlab = "Ave. Capital Run Length")
```

* There are a few lengths that are large causing this overall distribution to be quite skewed, so you may want to standardize the variable. This can be seen because of the large standard deviation, `r round(sd(training$capitalAve), 2)`.  

#### Standardizing  
* Subtract the mean from each variable then divide by the sd.  
```{r}
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)
```
* Standardizing a data set will make the mean nearly 0 and the sd 1 as it's now measuring z-scores  
* When evaluating on the test set one can only use parameters that were estiamted from the training set. In other words when we standardize the testing set with have to use the mean & sd of the training set. This will result in the mean and sd not be 1 and 0, respectively.  
```{r}
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve -
                  mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS)
sd(testCapAveS)
```

##### Standardizing with preProcess Function  
```{r}
preObj <- preProcess(training[,-58], #Passing all variables except SPAM/HAM
                     method = c("center", #Subtracts mean
                                "scale")) #Divides by sd
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
```
* You can then use this object returned by `preProcess` to pre process the test set  
```{r}
testCapAveS <- predict(preObj, testing[,-58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)
```

##### Standardizing with preProcess Argument  
```{r warning = FALSE}
set.seed(32343)
modelFit <- train(type ~., data = training,
                  preProcess = c("center", "scale"), 
                  method = "glm")
modelFit
```

##### Standardizing - Box-Cox Transforms  
* BoxCox transforms are a set of transforms that take continous data and try to make them look like normal data. They do this by estimating a set of parameters based on maximum likelyhood  
```{r}
preObj <- preProcess(training[,-58], method = c("BoxCox"))
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
par(mfrow = c(1,2))
hist(trainCapAveS); qqnorm(trainCapAveS)
```
  
* Note that there are still a stack of values at or around 0  
  + This is because BoxCox does not take care of repeated variables  

#### Imputing Data  
```{r}
set.seed(13343)

# Creating some NA values  
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size=1, prob = 0.05) == 1
training$capAve[selectNA] <- NA

# Impute and standardize  
preObj <- preProcess(training[,-58], 
                     method = "knnImpute") #K Nearest Neighbors Impute
capAve <- predict(preObj, training[,-58])$capAve

# Standardize true values  
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth - mean(capAveTruth))/sd(capAveTruth)

# How close was the imputting
quantile(capAve - capAveTruth)  #all
quantile((capAve - capAveTruth)[selectNA]) #only missing value
quantile((capAve - capAveTruth)[!selectNA]) #exclude missing
```

#### Notes & Further Reading  
* Training and test sets must be pre-processed in the same way  
* Test transformations will likely be imperfect  
  + Especially if the test/training sets are collected at different times or in different ways  
* Careful when transforming factor variables!  
* **[preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)**  

### Covariate Creation  
* Covariates are also called *predictors* or *features*, they are what variables are used and combined to predict the outcome you're looking at.  
* The raw data usually takes form of an image or text file so you want to turn that into a quantifiable predictor. The goal is to describe the data as much as possible while also compressing the key components of the data. In spam example we look at things like:  
  + Proportion of capital letters, `capitalAve`
  + Frequency of a given word, `you`, or character, `numDollar` (dollar signs).
* Sometimes after collecting the variables we want to transform the data to a more useful value, like squaring a value.  

#### Level 1, Raw Data -> Covariates  
* Depends heavily on application  
* The balancing act is summarization vs. information loss  
* Examples:  
  + Text files: frequency fo words, frequency of phrases (**[Google ngrams](https://books.google.com/ngrams)**), frequency of capital letters.  
  + Images: Edges, corners, blobs, ridges (**[computer vision feature detection](https://bit.ly/2VWq2Id)**)  
  + Webpages: Number and type of images, position fo elements, colors, videos (**[A/B Testing](http://en.wikipedia.org/wiki/A/B_testing)**)  
  + People: Height, weight, hair color, sex, country of orgin.  
* The more knowledge of the system you have the better the job you will do at extracting features.  
* When in doubt, err on the side of more features  
* Can be automated, but use caution!  

#### Level 2, Tidy Covariates -> New Covariates  
* More necessary for some methods (regression, support vector machines (svms)) than for others (classification trees).  
* Deciding how to create them should be done *only on the training set*  
  + When applying the prediction you'll make these same mutations  
* The best approach is through exploratory analysis (plotting/tables)  
* New covariates should be added to data frames with recognizable names  

#### Wage Example  
```{r}
library(caret)
library(ISLR); data(Wage)
set.seed(1618033)
inTrain <- createDataPartition(y = Wage$wage,
                               p = 0.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
```

##### Adding "Dummy Variables"  
```{r}
table(training$jobclass)
```
* Since jobs are either `Industrial` or `Information` we may want to melt this factor variable into two seperate logical columns  
```{r}
dummies <- dummyVars(wage ~ jobclass, data = training)
head(predict(dummies, newdata = training))
```

##### Removing Zero Covariates  
```{r}
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
```

* A `freqRatio` near 0 indicates that it rarely has a unique value.  
* `TRUE` in either `zeroVar` or `nzv` indicates that you likely can remove that variable without losing any context.  

##### Spline Basis  
```{r}
library(splines)
bsBasis <- bs(training$age, df = 3)
head(bsBasis,16)
```
  
* `bs` is a basis function that creates a polynomial variables of the `df` degree, with each column being a higher order.  
* Including these covariates allows for more curved of a model fit  
```{r}
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 10, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = "#FF0000", pch = 19, cex = 0.5)
```

###### Splines on the Test Set  
* You have to create the predictions on the test set with the exact same percedure you used on the training set.  
* As such we have to predict what the variables of the testing set will be with the prediction from the training set.  
```{r}
predict(bsBasis, age = testing$age)
```

#### Notes and Further Reading  
* Level 1 feature creation (raw data to covariates)  
  + Science is key. Google "feature extraction for [data type]"  
  + Err on overcreation of features, you can always filter them out later in the process.  
  + In some applications (images, voices) automated feature creation is possible/necessary  
  + **[A PDF on deep learning to auto create features for images and voice](http://www.cs.nyu.edu/~yann/talks/lecun-ranzato-icml2013.pdf)**  
* Level 2 feature creation (covariates to new covariates)  
  + The function `preProcess` in `caret` will handle some preprocessing  
  + Create new covariates if you think they will improve fit  
  + Use exploratory analysis on the training set for creating them  
  + Be careful about overfitting!  
* **[preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)**  
* If you want to fit spline models, use the `gam` method in the `caret` package which allows smoothing fo multiple variables.  
* More on feature creation/data tidying in the **[Getting and Cleaning Data course](https://github.com/PhiPrime/LassoingDataNotes)**


### Preprocessing with Principal Components Analysis (PCA)  
* Often there will be multiple quantitative variables that are highly correlated, as such one may want to create a summary that contains most of the information from those quantitative variables  

#### Correlated predictors  
```{r}
library(caret)
library(kernlab); data(spam)
set.seed(1618033)
inTrain <- createDataPartition(y = spam$type,
                               p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]

M <- abs(cor(training[,-58]))#Remove SPAM/HAM value

#Every variable has a correlation of 
# 1 with itself so we remove those
diag(M) <- 0 
which(M > 0.8, arr.ind = TRUE)
```
* `num415`(col 32) and `num857` (col 34) often appear together. It seems the word `direct` does too but we'll just look at the numbers in this example  
```{r}
rows <- c(34, 32, 40)
n <- names(spam)[rows]
n[1:2]
plot(spam[,rows[1]], spam[,rows[2]], xlab = n[1], ylab = n[2])
```

* Plotting them with eachother shows that they seem to be almost exactly 1-to-1. As such we could probably use less information to convey that both variables are present.  

```{r}
#Looking at plots of that direct variable too
plot(spam[,rows[1]], spam[,rows[3]], xlab = n[1], ylab = n[3])
plot(spam[,rows[2]], spam[,rows[3]], xlab = n[2], ylab = n[3])
```

* `direct` may not be as strongly correlated but seeing the simularity in those two graphs builds a greater case for `num415` and `num857` being highly correlated.  

##### We Could Rotate the Plot  
* We'll be using `0.71` because it is a result from PCA later.  
```{r}
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X,Y)
```
* We can see there is not a lot of variation on the Y axis, the difference  
* Most of the variability occurs on the X axis, the sum, as such we may want to use the sum as a predictor since it captures most of the information from the two variables.  

#### Basic PCA Idea  
* We might not need every predictor  
* A weighted combination fo predictors might be better  
* We should pick this combination to capture the "most information" possible  
* Benefits  
  + Reduced numeber of predictors  
  + Reduced noise (due to averaging)  

##### Related Probelms  
* You have multivariate variables $X_1, ..., X_n$ so $X_1 = (X_{11},...,X_{1m})$  
  + Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.  
  + If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the orginal data.  
* The first goal is **statistical** and the second goal is **data compression**.  

#### Related Solutions - PCA/SVD  
* **[I also have notes on this from the Exploratory Data Analysis course](https://github.com/PhiPrime/ExploratoryDataAnalysisNotes)**  
* SVD - Singular Value Decomposition  
  + If $X$ is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"  
$X = UDV^T$  
  + Where the columns of $U$ are orthogonal (left singular vectors), the columns of $V$ are orthogonal (right singular vectors) and $D$ is a diagonal matric (singular values).  
* PCA - Principal Component Analysis  
  + The principal components are equal tot eh right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables.  

#### Principal Components in R - prcomp  
```{r}
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1], prComp$x[,2])
```

* We can look at how `prcomp` is summing the variables  
```{r}
prComp$rotation
```
* Taking the coefficients of `PC1` & `PC2` for the sum and difference respectively  
* PCA allows one to perform this operation when dealing with more than just two variables  

#### PCA on SPAM data  
```{r}
typeColor <- ((spam$type == "spam") * 1 + 1)# Red if SPAM, Black if HAM

# Apply log_10 to make data look more gaussian
prComp <- prcomp(log10(spam[,-58]+1))

plot(prComp$x[,1], prComp$x[,2], col = typeColor, xlab = "PC1", ylab = "PC2")
```

#### PCA with caret  
```{r}
preProc <- preProcess(log10(spam[,-58] + 1), method = "pca", pcaComp = 2)
spamPC <- predict(preProc, log10(spam[,-58] + 1))
plot(spamPC[,1], spamPC[,2], col = typeColor)
```

#### Preprocessing with PCA  
```{r warning = FALSE}
preProc <- preProcess(log10(training[,-58] + 1), 
                      method = "pca", pcaComp = 2)
trainPC <- predict(preProc, log10(training[,-58] + 1))
modelFit <- train(y = training$type, x = trainPC, method = "glm")
testPC <- predict(preProc, #Have to use training preProc procedure for test
                  log10(testing[, -58] + 1))
confusionMatrix(testing$type, predict(modelFit, testPC)) #Find result
```

#### Alternative: Built in PCA to train (sets # of PCs)  
```{r warning = FALSE}
modelFit <- train(type ~ ., method = "glm", 
                  preProcess = "pca", data = training)
confusionMatrix(testing$type, predict(modelFit, testing))
```

#### Final Notes on PCs  
* Most useful for linear-type models  
* Can make it harder to itnerpret predictors  
* Watch out for outliers!  
  + Transform first (with logs/Box Cox)  
  + Plot predictors to identify problems  
* More info: **[Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)**  


## Predicting  
### Predicting with Regression  
* **[Covered more fully in my notes for Regression Models](https://github.com/PhiPrime/RegressionModelsNotes)**  

#### Key Ideas  
* Fit a Simple Regression Model  
* Plug in new covariates and multiply by the coefficients  
* Useful when the linear model is (nearly correct)
**Pros:**  
* Easy to implement  
* Easy to interpret  
**Cons:**  
* Often poor performance in nonlinear settings  

#### Example: Old Faithful Eruptions  
```{r}
data(faithful)
library(caret)
set.seed(333)
inTrain <- createDataPartition(y = faithful$waiting, p = 0.5, list = FALSE)

trainFaith <- faithful[inTrain, ]
testFaith <- faithful[-inTrain, ]
head(trainFaith)
```

* Plot of eruption duration versus waiting time  
```{r}
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "#0000FF",
     xlab = "Waiting", ylab = "Duration")
```

* One can see a line that may be able to go through these points that shows longer waiting correlates to longer erruptions  

#### Fit a Linear Model  
* Formula we fit is with an Eruption Duration, $ED$, and Wait Time, $WT$, and a gaussian distributed error term, $e$:  
$ED_i = b_0 + b_1WT_i+e_i$  
```{r}
lm1 <- lm(eruptions ~ waiting, data = trainFaith)
summary(lm1)
```

* These $b_0$ and $b_1$ values are given by the respective rows in the `Estimate` column.  
```{r}
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "#0000FF",
     xlab = "Waiting", ylab = "Duration")
lines(trainFaith$waiting, lm1$fitted, lwd = 3)
```

#### Predict a New Value  
* Explicitly  
```{r}
newValue <- 80
coef(lm1)[1] + coef(lm1)[2]*newValue
```

* With `predict`  
```{r}
#New value has to be named the same as explanatory variable
newValue <- data.frame(waiting = 80) 
predict(lm1, newValue)
```

#### Plot Predictions on the Test Set  
```{r}
plot(testFaith$waiting, testFaith$eruptions, pch = 19, col = "#0000FF",
     xlab = "Waiting", ylab = "Duration")
lines(testFaith$waiting, 
      predict(lm1, 
              newdata = testFaith), #Have to use info from train to predict
      lwd = 3)
```

#### Get Training/Test Set Errors  
```{r}
# Calculate RMSE on training  
sqrt(sum(
  (lm1$fitted - trainFaith$eruptions)^2))

# Calculate RMSE on test
sqrt(sum(
  (predict(lm1, newdata = testFaith) - testFaith$eruptions)^2))
```

#### Prediction Intervals  
```{r}
pred1 <- predict(lm1, newdata = testFaith, 
                 interval = "prediction") #Returns intervals
ord <- order(testFaith$waiting)
plot(testFaith$waiting, testFaith$eruptions, pch = 19, col = "#0000FF")
matlines(testFaith$waiting[ord], pred1[ord,], type= "l", 
         col = c(1,2,2), lty = c(1,1,1), lwd = 3)
```
* Red lines are the 95% CI of the prediction  

#### Same Process in caret Package  
```{r}
modFit <- train(eruptions ~ waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
```

#### Notes and Further Reading  
* Regression models with multiple covariates can be included  
* Often useful in combination with other models  
* Further Reading:  
  + **[Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)**  
  + **[Modern applied statistics with S](http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570)**  
  + **[Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)**  

### Predicting with Regression Multiple Covariates  
* We'll be using the `Wage` data  
```{r message = FALSE}
library(ISLR); data(Wage)
library(ggplot2)
library(caret)
```

* We're going to subset out the variable we're trying to predict, `logwage`  
```{r}
wage <- subset(Wage, select = -c(logwage))
summary(wage)
```

* Get training/test sets  
```{r}
set.seed(1618033)
inTrain <- createDataPartition(y = wage$wage, 
                               p = 0.7, list = FALSE)
training <- wage[inTrain, ]
testing <- wage[-inTrain, ]
dim(training); dim(testing)
```

* Feature plot, sometimes helpful to detect trends  
```{r}
featurePlot(x = training[,c("age", "education", "jobclass")],
            y = training$wage,
            plot = "pairs")
```

* This lecture is largely covering some exploratory analysis done before on these data so I'm going to put them all here.  
```{r}
qplot(age, wage, data = training)
qplot(age, wage, colour = jobclass, data = training)
qplot(age, wage, colour = education, data = training)
```

#### Fit a Linear Model  
$ED_i = b_0 + b_1age + b_2I(Jobclass_i = "Information") + \sum_{k=1}^4\eta_kI(education_i=levelk)$  
* $I(Jobclass_i= "Information")$ is how an indicator variable is indicated in mathematical notation. Likewise for the $\eta_kI(education_i=levelk)$ which is checking for the 5 catagories (only 4 DF)  
```{r}
modFit <- train(wage ~ age + jobclass + education,
                method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
```

#### Diagnostics  
```{r}
plot(finMod, 1, pch = 19, cex = 0.5, col = "#00000010")
```

* We can plot by variables not included in the model to help identify further outliers  
```{r}
qplot(finMod$fitted, finMod$residuals, colour = race, data = training)
```

* Plotting by index can sometimes give insight to entry issues  
```{r}
plot(finMod$residuals, pch = 19)
```

* If a trend was seen here it could suggest there is a varaible missing from the model.  

#### Plotting Predicted Versus Truth in Test Set  
```{r}
pred <- predict(modFit, testing)
qplot(wage, pred, colour = year, data = testing)
```
* Ideally we'd have an identity line that our prediction matched the truth, obviously that typically won't be the case  
* This is more used as a "post-mortum" tool on your analysis as adjusting after this would be like using the test to train.  

```{r warning = FALSE}
#Using all covariates  
modFitAll <- train(wage ~ ., data = training, method = "lm")
pred <- predict(modFitAll, testing)
qplot(wage, pred, data = testing)
```
* Useful if you don't want to do some sort of model selection in advance  

#### Notes and Further Reading  
* Regression models are often useful in combinaiton with other models.  
* Further Reading:  
  + **[Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)**
  + **[Modern applied statistics with S](http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570)**
  + **[Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)**



## Quiz 2  

1. Load the Alzheimer's disease data:  
```{r}
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
```

* What commands will create non-overlapping training and test sets with about 50% of the observations assigned to each?  
```{r}
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
```

2. Load (and subset) the cement data with the following commands:  
```{r}
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[inTrain,]
testing = mixtures[-inTrain,]
```

Make a plot of the outcome (CompressiveStrength) versus the index of the samples. Color by each of the variables in the data set.  
```{r message = FALSE}
library(tidyverse)
library(RColorBrewer)
plot <- ggplot(training, 
               aes(as.numeric(row.names(training)), CompressiveStrength)) +
  scale_color_gradient(low = "#A43D18", high = "#5BC2E7")
plot + geom_point(aes(col = Cement))
plot + geom_point(aes(col = BlastFurnaceSlag))
plot + geom_point(aes(col = FlyAsh))
plot + geom_point(aes(col = Water))
plot + geom_point(aes(col = Superplasticizer))
plot + geom_point(aes(col = CoarseAggregate))
plot + geom_point(aes(col = FineAggregate))
plot + geom_point(aes(col = Age))
plot + geom_point() + geom_smooth()
```


What do you notice in these plots?  
* There is a non-random pattern in the plot of the outcome versus index that does not appear to be perfectly explained by any predictor suggesting a variable may be missing.  

3. Make a histogram and confirm the `Superplasticizer` variable is skewed.  
```{r}
plot <- ggplot(training, aes(Superplasticizer)) + 
  geom_histogram(bins = 50)
plot
```

Normally you might use the log transform to try to make the data more symmetric. Why would that be a poor choice for this variable?  
```{r}
plot <- ggplot(training, aes(log(Superplasticizer + 1))) + 
  geom_histogram(bins = 50)
plot
```
  
* The log transform does not reduce the skewness of the non-zero values of `Superplasticizer`  

4. Load (and subset) the Alzheimer's disease data:  
```{r}
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
```
Find all the predictor variables in the training set that begin with IL.  
```{r}
begIL <- grepl("^IL", colnames(training))
```

Perform principal componetns on these variables with the `preProcess()` function from the `caret` package. Calculate the number of principal components needed to capture 80% of the variance. How many are there?  
```{r}
ILset <- training[,begIL]
preProc <- preProcess(ILset, method = "pca", thresh = 0.8)
preProc
```

5. With the Alzheimer's training data set consisting of only the predictors with variable names beginning with IL and the diagnosis. Build two predictive models, one using the predictors as they are and one using PCA with principal components explaining 80% of the variance in the predictors. Use `method = "glm"` in the train function.  
```{r warning = FALSE}
trainIL <- cbind(diagnosis = training$diagnosis, ILset)
preProc <- preProcess(trainIL, method = "pca", thresh = 0.8)
ILPC <- predict(preProc, trainIL)

NonPCA <- train(diagnosis ~., data = trainIL, method = "glm")
WithPCA <- train(diagnosis ~., data = trainIL, method = "glm",
                 preProcess = "pca", trControl = trainControl(
                   preProcOptions = list(thresh = 0.8)))
```

What is the accuracy of each method in the test set? Which is more accurate?  
```{r}
nopcres <- confusionMatrix(testing$diagnosis, 
                           predict(NonPCA, testing))
pcres <- confusionMatrix(testing$diagnosis, 
                         predict(WithPCA, testing))
round(nopcres$overall[1], 2)
round(pcres$overall[1], 2)
```


  

# Predicting with Trees, Random Forests, & Model Based Predictions  
## Trees  
### Predicting with Trees  
#### Key Ideas  
* Iteratively split variables into groups  
* Evaluate "homogeneity" of outcome within each group  
* Split again if necessary until groups are homogeneous or small enough  
**Pros:**  
* Easy to interpret  
* Better performance in nonlinear settings  
**Cons:**  
* Without pruning/cross-validation can lead to overfitting  
* Harder to estimate uncertainty  
* Results may be variable  

#### Example Tree  
![Decision Tree: The Obama-Clinton Divide](./Images/Obama-Clinton_Decision_Tree.jpg)  
  
* This tree appeared in the NY Times during the 2008 elections when Obama was running against Clinton for the democratic election  
* The tree asks the most likely split that would result from a question first, then asked any further questions until all the counties were subsetted.  

#### Basic Algorithm  
1. Start with all variables in one group  
2. Find the variable/split that best separates the outcomes  
3. Divide the data into two groups ("leaves") on that split ("node")  
4. Within each split, find the best variable/split that separates the outcomes  
5. Continue until the groups are too small or sufficiently "pure"  

#### Measures of Impurity  
* Impurity has different measurements that are based on the following formula:  
$\hat{p}_{mk} = \frac{1}{N_m} \sum_{x_i\space in\space Leaf\space m}\mathbb{1}(y_i = k)$  
* Where:  
  + $N_m$ is the number of objects in leaf $m$  
  + $\sum_{x_i\space in\space Leaf\space m}\mathbb{1}(y_i = k)$ is the count of how many times that class $k$ appears in leaf $m$  

**Misclassification Error:**  
$1-\hat{p}_{mk(m)}$  
* Where $k(m)$ is the most common k  
* 0 = perfect purity  
* 0.5 = no purity (perfectly balanced means no homogenity)  

**Gini Index:**  
$\sum_{k\neq k'}\hat{p}_{mk} \times \hat{p}_{mk'} = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})=1-\sum_{k=1}^Kp_{mk}^2$  
* Essentially 1 minus the sum of the squared probability that an object belongs to any of the different classes  
* 0 = perfect purity  
* 0.5 = no purity  

**Deviance/Information Gain:**  
$-\sum{k=1}^K\hat{p}_{mk}\log_2\hat{p}_{mk}$  
* Deviance uses log base $e$, Information is as above  
* 0 = perfect purity  
* 1 = no purity  
  
* **[Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)**  

##### Example  
* Say we have an object of 16 points, 15 blue & 1 red  
  + This **Misclassification Error** would be $1-\frac{15}{16} = 1/16 = 0.0625$  
  + The **Gini Index** would be $1 - (\frac{1}{16}^2 + \frac{15}{16}^2) \approx 0.1172$  
  + The **Information Gain** would be $-[\frac{1}{16}\times \log_2(\frac{1}{16}) + \frac{15}{16}\times \log_2(\frac{15}{16})] \approx 0.3373$  
  
* Say we have an object of 16 points, 8 blue & 8 red  
  + This **Misclassification Error** would be $1-\frac{8}{16} = 8/16 = 0.5$  
  + The **Gini Index** would be $1-(\frac{8}{16}^2 + \frac{8}{16}^2) = 0.5$  
  + The **Information Gain** would be $-[\frac{8}{16}\times \log_2(\frac{8}{16}) + \frac{8}{16}\times \log_2(\frac{8}{16})] = 1$  
  
#### Example: Iris Data  
```{r message = FALSE}
data(iris)
library(tidyverse)
```
```{r}
names(iris)
table(iris$Species) #What we're predicting
```
```{r}
#Create training/test sets  
set.seed(1618033)
inTrain <- createDataPartition(y = iris$Species,
                               p = 0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
```
```{r}
#Plot Iris petal/sepal width
qplot(Petal.Width, Sepal.Width, colour = Species, data = training)
```
  
* It can be seen here that there is a distinct cluster for `setosa`, and a semi-distinct cluster for `versicolor` & `virginica`  
```{r}
#Training
library(caret)
modFit <- train(Species ~., 
                method = "rpart", #R package for reg. & classification trees
                data = training)
```

##### Viewing Model
```{r}
print(modFit$finalModel)
```
* 1-7 gives a decision tree for determining the prediction.  
2) If `Petal.Length` is less than 2.45 all of these belong to `setosa`  
3) If `Petal.Length` is greater than or equal to 2.45 it's a 50/50 split between the other two species  
6) If `Petal.Length` is less than 4.85 there is about a 0.97 probability that specis is `versicolor`  

##### Plotting Tree  
```{r}
#Dendrogram
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = 0.8)
```

```{r}
#Prettier version
library(rattle)
fancyRpartPlot(modFit$finalModel)
```

##### Predicting New Values  
```{r}
#Gives factors since that's what it was predicting
predict(modFit, newdata = testing)
```

#### Notes and Further Resources  
* Classification trees are non-linear models  
  + They use interactions between variables  
  + Data transformations may be less important (monotone transformations, doesn't change order of values; will give same data splits)  
  + Trees can also be used for regression problems (continuous outcome)  
* Note that there are multiple tree building options in R, both in the caret package (`party`, `rpart`) and out of the caret package (`tree`)  
* Further Resources  
  + **[Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)**
  + **[Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)**
  + **[Classification and regression trees](http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418)**

### Bagging (Bootstrap Aggregating)  
* When you fit complicated models, sometimes averaging those models together will give you a smoother model fit that gives you a better balance between potential bias & varaince  

#### Key Idea  
* Basic idea:  
  1. Resample cases (with replacement) and recalculate predictions  
  2. Average or majority vote  
* Similar bias to fitting any of those models individually  
* Reduced variance  
* More useful for non-linear functions  

#### Example: Ozone data  
* Note: Ozone data was in the `ElemStatLearn` package which has been archived by CRAN, as such the following code chunk is needed to install the archived version of the package  
```{r eval = FALSE}
url <- "http://cran.r-project.org/src/contrib/Archive/ElemStatLearn/ElemStatLearn_2015.6.26.tar.gz"
pkgFile <- "ElemStatLearn_2015.6.26.tar.gz"
download.file(url = url, destfile = pkgFile)
#There are no dependencies for ElemStatLearn
install.packages(pkgs = pkgFile, type = "source", repos = NULL)
```

```{r}
library(ElemStatLearn)
data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
```

* We're going to try to predict `temperature` as a function of `ozone`  

#### Bagged Loess  
* We're going to sample with replacement from the entire data set  
* Store & order it by `ozone`  
* Create a loess (lo-ess) model  
* Then predict 1:155 using the loess model and store it in our matrix, `ll`.  
* We'l repeat this 10 times  
```{r}
set.seed(1618033)
ll <- matrix(NA, nrow = 10, ncol = 155)
for (i in 1:10) {
  ss <- sample(1:dim(ozone)[1], replace = TRUE)
  ozone0 <- ozone[ss,]
  ozone0 <- ozone0[order(ozone0$ozone),]
  loess0 <- loess(temperature ~ ozone, data = ozone0, span = 0.2)
  ll[i,] <- predict(loess0, newdata = data.frame(ozone = 1:155))
}

plot(ozone$ozone, ozone$temperature, pch = 19, cex = 0.5)
for (i in 1:10) {lines(1:155, ll[i,], col = "#88888888", lwd = 2)}
lines(1:155, apply(ll, 2, mean), col = "#FF0000", lwd = 2)
```
  
* Grey lines are the 10 fits with the resampled data set.  
  + They capture a lot of the noise from the dataset but maybe a little *too* much  
* The red line is the average of the 10 fits.  

#### Bagging in caret  
* Some models perform bagging for you, in the `train` function consider the `method` parameters:  
  + `bagEarth`  
  + `treebag`  
  + `bagFDA`  
* Alternatively you can bag any model you choose using the `bag` function  
  + A bit of an advanced use so read the documentation if doing this  
```{r warning=FALSE}
predictors = data.frame(ozone = ozone$ozone)
temperature = ozone$temperature  
treebag <- bag(predictors, temperature, B = 10, #Number of replications
               bagControl = bagControl(#Informs how the model is to be fit
#Function to fit the model, could be `train`
                        fit = ctreeBag$fit,
#How new values will be predicted
                        predict = ctreeBag$pred,
#How predictions will be put together, such as averaging
                        aggregate = ctreeBag$aggregate))

#Plotting custom Bagging
plot(ozone$ozone, temperature, col = "#888888", pch = 19)
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors), 
       pch = 19, col = "#FF0000")
points(ozone$ozone, predict(treebag, predictors), pch = 19, col = "#0000FF")
```
* Red values is fit from first regression tree  
* Blue is the average of the 10 models

#### Parts of Bagging  
```{r}
ctreeBag$fit
```
* returns the `ctree` function  
```{r}
ctreeBag$pred
```
* creates `rawProbs` and uses that to make `probMatrix`, it then returns the observed levels or the response, depending on what's applicable for the object  
```{r}
ctreeBag$aggregate
```
* takes predictions and puts them together (aggregates) in some way. Taking the median by default  

#### Notes and Further Resources  
* Bagging is most useful for nonlinear models  
* Often used with trees - an extension is random forests  
* Several models use bagging in `caret`'s `train` function  
* Further resources:  
  + **[Bagging](http://en.wikipedia.org/wiki/Bootstrap_aggregating)**  
  + **[PDF on Bagging and boosting](http://stat.ethz.ch/education/semesters/FS_2008/CompStat/sk-ch8.pdf)**  
* **[Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)**  




**Reminder to Commit (08), Delete this line** ***AFTER*** **Committing**  

## Random Forests  
### Random Forests  
1. Bootstrap sample  
2. At each split, bootstrap variables  
* As such only a subset of variables are considered at each split  
3. Grow multiple trees and vote  
**Pros:**  
* Accuracy  
**Cons:**  
* Speed  
* Interpretability  
* Overfitting  



### Boosting  

**Reminder to Commit (09), Delete this line** ***AFTER*** **Committing**  

## Model Baded Predictions  
### Model Based Predictions  

**Reminder to Commit (10), Delete this line** ***AFTER*** **Committing**  

## Quiz 3  

**Reminder to Commit (Q3), Delete this line** ***AFTER*** **Committing**  

# Regularized Regression and Combining Predictors  
## Regularized Regression  
## Combining Predictors  

**Reminder to Commit (11), Delete this line** ***AFTER*** **Committing**  

## Forecasting  
## Unsupervised Prediction  

**Reminder to Commit (12), Delete this line** ***AFTER*** **Committing**  

## Quiz 4  

**Reminder to Commit (Q4), Delete this line** ***AFTER*** **Committing**  

# Course Project  

**Reminder to Commit (P1), Delete this line** ***BEFORE*** **Committing**  
